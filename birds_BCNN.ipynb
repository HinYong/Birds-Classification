{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCNN, self).__init__()\n",
    "        # VGG16的卷积层和池化层\n",
    "        self.features = torchvision.models.vgg16(pretrained=True).features\n",
    "        self.features = nn.Sequential(*list(self.features.children())[:-1])  # 去除最后的池化层\n",
    "        # 线性分类层\n",
    "        self.fc = nn.Linear(512 * 512, 200)\n",
    "        # 冻结以前的所有层\n",
    "        for param in self.features.parameters():\n",
    "            param.requres_grad = False\n",
    "        # 初始化全连接层\n",
    "        nn.init.kaiming_normal_(self.fc.weight.data)\n",
    "        if self.fc.bias is not None:\n",
    "            nn.init.constant_(self.fc.bias.data, val=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.size()[0]\n",
    "        x = self.features(x)\n",
    "        x = x.view(N, 512, 28 * 28)  # 改变tensor形状\n",
    "        x = torch.bmm(x, torch.transpose(x, 1, 2)) / (28 * 28)  # 双线性池化层，使用bmm函数对所有位置L进行批量矩阵乘计算，得到512*512的矩阵\n",
    "        x = x.view(N, 512 * 512)  # 将bilinear matrix拉直为bilinear vector\n",
    "        x = torch.sqrt(x + 1e-5)  # 将bilinear vector开方\n",
    "        x = torch.nn.functional.normalize(x)  # 将bilinear vector归一化\n",
    "        x = self.fc(x)  # 传入全连接层\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 64, 448, 448]           1,792\n",
      "              ReLU-2         [32, 64, 448, 448]               0\n",
      "            Conv2d-3         [32, 64, 448, 448]          36,928\n",
      "              ReLU-4         [32, 64, 448, 448]               0\n",
      "         MaxPool2d-5         [32, 64, 224, 224]               0\n",
      "            Conv2d-6        [32, 128, 224, 224]          73,856\n",
      "              ReLU-7        [32, 128, 224, 224]               0\n",
      "            Conv2d-8        [32, 128, 224, 224]         147,584\n",
      "              ReLU-9        [32, 128, 224, 224]               0\n",
      "        MaxPool2d-10        [32, 128, 112, 112]               0\n",
      "           Conv2d-11        [32, 256, 112, 112]         295,168\n",
      "             ReLU-12        [32, 256, 112, 112]               0\n",
      "           Conv2d-13        [32, 256, 112, 112]         590,080\n",
      "             ReLU-14        [32, 256, 112, 112]               0\n",
      "           Conv2d-15        [32, 256, 112, 112]         590,080\n",
      "             ReLU-16        [32, 256, 112, 112]               0\n",
      "        MaxPool2d-17          [32, 256, 56, 56]               0\n",
      "           Conv2d-18          [32, 512, 56, 56]       1,180,160\n",
      "             ReLU-19          [32, 512, 56, 56]               0\n",
      "           Conv2d-20          [32, 512, 56, 56]       2,359,808\n",
      "             ReLU-21          [32, 512, 56, 56]               0\n",
      "           Conv2d-22          [32, 512, 56, 56]       2,359,808\n",
      "             ReLU-23          [32, 512, 56, 56]               0\n",
      "        MaxPool2d-24          [32, 512, 28, 28]               0\n",
      "           Conv2d-25          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-26          [32, 512, 28, 28]               0\n",
      "           Conv2d-27          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-28          [32, 512, 28, 28]               0\n",
      "           Conv2d-29          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-30          [32, 512, 28, 28]               0\n",
      "           Linear-31                  [32, 200]      52,429,000\n",
      "================================================================\n",
      "Total params: 67,143,688\n",
      "Trainable params: 67,143,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 73.50\n",
      "Forward/backward pass size (MB): 4837.95\n",
      "Params size (MB): 256.13\n",
      "Estimated Total Size (MB): 5167.58\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\torchsummary\\torchsummary.py:93: RuntimeWarning: overflow encountered in long_scalars\n",
      "  total_output += np.prod(summary[layer][\"output_shape\"])\n"
     ]
    }
   ],
   "source": [
    "# 输出网络模型结构\n",
    "summary(BCNN(), input_size=(3, 448, 448), batch_size=32, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = 0.9  # 数据集划分比\n",
    "BATCH_SIZE = 32  # 一次抓取的样本数量\n",
    "dataset = './data/images'\n",
    "\n",
    "\n",
    "class DatasetFromSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "def data_process():\n",
    "    train_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(size=448),  \n",
    "        torchvision.transforms.RandomHorizontalFlip(),  # 随机翻转\n",
    "        torchvision.transforms.RandomCrop(size=448),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        # 归一化\n",
    "        torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                         std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    test_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(size=448),\n",
    "        torchvision.transforms.RandomCrop(size=448),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        # 归一化\n",
    "        torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                         std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    # 生成训练集与测试集\n",
    "    data = torchvision.datasets.ImageFolder(root=dataset)\n",
    "    data_len = len(data)\n",
    "    train_len = int(train_val_split * data_len)\n",
    "    test_len = data_len - train_len\n",
    "    train_subset, test_subset = random_split(data, [train_len, test_len])\n",
    "    train_data = DatasetFromSubset(train_subset, transform=train_transforms)\n",
    "    test_data = DatasetFromSubset(test_subset, transform=test_transforms)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=0,\n",
    "                                               pin_memory=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=0,\n",
    "                                              pin_memory=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载数据\n",
    "train_loader, test_loader = data_process()\n",
    "\n",
    "BASE_LEARNING_RATE = 0.05\n",
    "EPOCHS = 100\n",
    "WEIGHT_DECAY = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "========== epoch: [1/100] ==========\n",
      "Epoch [1/100], Step [100/332], Training Loss: 5.2818 | Training accuracy:  0.938%\n",
      "Epoch [1/100], Step [200/332], Training Loss: 5.2501 | Training accuracy:  1.125%\n",
      "Epoch [1/100], Step [300/332], Training Loss: 5.2040 | Training accuracy:  3.323%\n",
      "Epoch:1 Training Loss:5.256 Acc: 4.044\n",
      "--- cost time: 158.5514s ---\n",
      "*************** test ***************\n",
      "test_loss: 5.193 | test_acc:  9.584%\n",
      "************************************\n",
      "\n",
      "========== epoch: [2/100] ==========\n",
      "Epoch [2/100], Step [100/332], Training Loss: 5.1625 | Training accuracy: 14.031%\n",
      "Epoch [2/100], Step [200/332], Training Loss: 5.1403 | Training accuracy: 15.031%\n",
      "Epoch [2/100], Step [300/332], Training Loss: 5.0594 | Training accuracy: 16.646%\n",
      "Epoch:2 Training Loss:5.144 Acc: 17.268\n",
      "--- cost time: 158.9415s ---\n",
      "*************** test ***************\n",
      "test_loss: 5.117 | test_acc: 18.745%\n",
      "************************************\n",
      "\n",
      "========== epoch: [3/100] ==========\n",
      "Epoch [3/100], Step [100/332], Training Loss: 5.0510 | Training accuracy: 27.000%\n",
      "Epoch [3/100], Step [200/332], Training Loss: 5.0115 | Training accuracy: 25.031%\n",
      "Epoch [3/100], Step [300/332], Training Loss: 5.0096 | Training accuracy: 26.031%\n",
      "Epoch:3 Training Loss:5.036 Acc: 26.402\n",
      "--- cost time: 158.9718s ---\n",
      "*************** test ***************\n",
      "test_loss: 4.966 | test_acc: 25.869%\n",
      "************************************\n",
      "\n",
      "========== epoch: [4/100] ==========\n",
      "Epoch [4/100], Step [100/332], Training Loss: 4.9304 | Training accuracy: 34.062%\n",
      "Epoch [4/100], Step [200/332], Training Loss: 4.8947 | Training accuracy: 32.328%\n",
      "Epoch [4/100], Step [300/332], Training Loss: 4.8984 | Training accuracy: 32.896%\n",
      "Epoch:4 Training Loss:4.929 Acc: 33.387\n",
      "--- cost time: 158.8835s ---\n",
      "*************** test ***************\n",
      "test_loss: 4.804 | test_acc: 29.941%\n",
      "************************************\n",
      "\n",
      "========== epoch: [5/100] ==========\n",
      "Epoch [5/100], Step [100/332], Training Loss: 4.8429 | Training accuracy: 39.344%\n",
      "Epoch [5/100], Step [200/332], Training Loss: 4.7684 | Training accuracy: 36.281%\n",
      "Epoch [5/100], Step [300/332], Training Loss: 4.7943 | Training accuracy: 36.542%\n",
      "Epoch:5 Training Loss:4.826 Acc: 37.261\n",
      "--- cost time: 158.9269s ---\n",
      "*************** test ***************\n",
      "test_loss: 4.734 | test_acc: 34.690%\n",
      "************************************\n",
      "\n",
      "========== epoch: [6/100] ==========\n",
      "Epoch [6/100], Step [100/332], Training Loss: 4.7375 | Training accuracy: 44.812%\n",
      "Epoch [6/100], Step [200/332], Training Loss: 4.7244 | Training accuracy: 42.922%\n",
      "Epoch [6/100], Step [300/332], Training Loss: 4.7917 | Training accuracy: 43.333%\n",
      "Epoch:6 Training Loss:4.725 Acc: 43.736\n",
      "--- cost time: 158.8484s ---\n",
      "*************** test ***************\n",
      "test_loss: 4.650 | test_acc: 37.829%\n",
      "************************************\n",
      "\n",
      "========== epoch: [7/100] ==========\n",
      "Epoch [7/100], Step [100/332], Training Loss: 4.7494 | Training accuracy: 50.094%\n",
      "Epoch [7/100], Step [200/332], Training Loss: 4.6661 | Training accuracy: 48.766%\n",
      "Epoch [7/100], Step [300/332], Training Loss: 4.5573 | Training accuracy: 48.521%\n",
      "Epoch:7 Training Loss:4.627 Acc: 48.779\n",
      "--- cost time: 159.0946s ---\n",
      "*************** test ***************\n",
      "test_loss: 4.502 | test_acc: 40.882%\n",
      "************************************\n",
      "\n",
      "========== epoch: [8/100] ==========\n",
      "Epoch [8/100], Step [100/332], Training Loss: 4.5129 | Training accuracy: 50.875%\n",
      "Epoch [8/100], Step [200/332], Training Loss: 4.5088 | Training accuracy: 50.547%\n",
      "Epoch [8/100], Step [300/332], Training Loss: 4.4824 | Training accuracy: 50.052%\n",
      "Epoch:8 Training Loss:4.531 Acc: 50.118\n",
      "--- cost time: 158.6786s ---\n",
      "*************** test ***************\n",
      "test_loss: 4.572 | test_acc: 41.221%\n",
      "************************************\n",
      "\n",
      "========== epoch: [9/100] ==========\n",
      "Epoch [9/100], Step [100/332], Training Loss: 4.5504 | Training accuracy: 53.188%\n",
      "Epoch [9/100], Step [200/332], Training Loss: 4.3448 | Training accuracy: 52.453%\n",
      "Epoch [9/100], Step [300/332], Training Loss: 4.3759 | Training accuracy: 52.167%\n",
      "Epoch:9 Training Loss:4.438 Acc: 52.135\n",
      "--- cost time: 159.2996s ---\n",
      "*************** test ***************\n",
      "test_loss: 4.415 | test_acc: 43.087%\n",
      "************************************\n",
      "\n",
      "========== epoch: [10/100] ==========\n",
      "Epoch [10/100], Step [100/332], Training Loss: 4.4658 | Training accuracy: 55.531%\n",
      "Epoch [10/100], Step [200/332], Training Loss: 4.4491 | Training accuracy: 55.359%\n",
      "Epoch [10/100], Step [300/332], Training Loss: 4.3258 | Training accuracy: 55.260%\n",
      "Epoch:10 Training Loss:4.349 Acc: 55.142\n",
      "--- cost time: 159.4093s ---\n",
      "*************** test ***************\n",
      "test_loss: 4.257 | test_acc: 43.681%\n",
      "************************************\n",
      "\n",
      "========== epoch: [11/100] ==========\n",
      "Epoch [11/100], Step [100/332], Training Loss: 4.3663 | Training accuracy: 57.344%\n",
      "Epoch [11/100], Step [200/332], Training Loss: 4.4567 | Training accuracy: 56.062%\n",
      "Epoch [11/100], Step [300/332], Training Loss: 4.1375 | Training accuracy: 55.906%\n",
      "Epoch:11 Training Loss:4.262 Acc: 56.066\n",
      "--- cost time: 159.4550s ---\n",
      "*************** test ***************\n",
      "test_loss: 4.167 | test_acc: 45.123%\n",
      "************************************\n",
      "\n",
      "========== epoch: [12/100] ==========\n",
      "Epoch [12/100], Step [100/332], Training Loss: 4.2127 | Training accuracy: 60.281%\n",
      "Epoch [12/100], Step [200/332], Training Loss: 4.1697 | Training accuracy: 58.438%\n",
      "Epoch [12/100], Step [300/332], Training Loss: 4.2708 | Training accuracy: 58.271%\n",
      "Epoch:12 Training Loss:4.176 Acc: 58.309\n",
      "--- cost time: 159.5718s ---\n",
      "*************** test ***************\n",
      "test_loss: 4.232 | test_acc: 45.632%\n",
      "************************************\n",
      "\n",
      "========== epoch: [13/100] ==========\n",
      "Epoch [13/100], Step [100/332], Training Loss: 3.9931 | Training accuracy: 58.562%\n",
      "Epoch [13/100], Step [200/332], Training Loss: 4.2189 | Training accuracy: 57.656%\n",
      "Epoch [13/100], Step [300/332], Training Loss: 4.0152 | Training accuracy: 57.729%\n",
      "Epoch:13 Training Loss:4.095 Acc: 58.375\n",
      "--- cost time: 159.5770s ---\n",
      "*************** test ***************\n",
      "test_loss: 4.134 | test_acc: 48.176%\n",
      "************************************\n",
      "\n",
      "========== epoch: [14/100] ==========\n",
      "Epoch [14/100], Step [100/332], Training Loss: 4.0981 | Training accuracy: 60.969%\n",
      "Epoch [14/100], Step [200/332], Training Loss: 3.8290 | Training accuracy: 60.391%\n",
      "Epoch [14/100], Step [300/332], Training Loss: 4.0333 | Training accuracy: 60.646%\n",
      "Epoch:14 Training Loss:4.015 Acc: 60.599\n",
      "--- cost time: 159.3240s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.885 | test_acc: 50.466%\n",
      "************************************\n",
      "\n",
      "========== epoch: [15/100] ==========\n",
      "Epoch [15/100], Step [100/332], Training Loss: 4.0100 | Training accuracy: 63.281%\n",
      "Epoch [15/100], Step [200/332], Training Loss: 3.8807 | Training accuracy: 62.953%\n",
      "Epoch [15/100], Step [300/332], Training Loss: 3.9734 | Training accuracy: 62.833%\n",
      "Epoch:15 Training Loss:3.936 Acc: 63.013\n",
      "--- cost time: 159.2041s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.839 | test_acc: 50.382%\n",
      "************************************\n",
      "\n",
      "========== epoch: [16/100] ==========\n",
      "Epoch [16/100], Step [100/332], Training Loss: 3.9410 | Training accuracy: 63.125%\n",
      "Epoch [16/100], Step [200/332], Training Loss: 3.9222 | Training accuracy: 62.344%\n",
      "Epoch [16/100], Step [300/332], Training Loss: 3.9292 | Training accuracy: 62.344%\n",
      "Epoch:16 Training Loss:3.863 Acc: 62.343\n",
      "--- cost time: 159.5496s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.748 | test_acc: 50.975%\n",
      "************************************\n",
      "\n",
      "========== epoch: [17/100] ==========\n",
      "Epoch [17/100], Step [100/332], Training Loss: 3.8742 | Training accuracy: 64.438%\n",
      "Epoch [17/100], Step [200/332], Training Loss: 4.0294 | Training accuracy: 64.656%\n",
      "Epoch [17/100], Step [300/332], Training Loss: 3.7877 | Training accuracy: 64.333%\n",
      "Epoch:17 Training Loss:3.789 Acc: 64.417\n",
      "--- cost time: 159.8172s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.664 | test_acc: 51.654%\n",
      "************************************\n",
      "\n",
      "========== epoch: [18/100] ==========\n",
      "Epoch [18/100], Step [100/332], Training Loss: 3.8666 | Training accuracy: 66.375%\n",
      "Epoch [18/100], Step [200/332], Training Loss: 3.6767 | Training accuracy: 65.203%\n",
      "Epoch [18/100], Step [300/332], Training Loss: 3.6399 | Training accuracy: 65.708%\n",
      "Epoch:18 Training Loss:3.717 Acc: 65.850\n",
      "--- cost time: 159.9323s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.529 | test_acc: 53.350%\n",
      "************************************\n",
      "\n",
      "========== epoch: [19/100] ==========\n",
      "Epoch [19/100], Step [100/332], Training Loss: 3.5161 | Training accuracy: 66.406%\n",
      "Epoch [19/100], Step [200/332], Training Loss: 3.8201 | Training accuracy: 65.625%\n",
      "Epoch [19/100], Step [300/332], Training Loss: 3.8022 | Training accuracy: 65.125%\n",
      "Epoch:19 Training Loss:3.651 Acc: 65.246\n",
      "--- cost time: 159.6824s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.550 | test_acc: 54.707%\n",
      "************************************\n",
      "\n",
      "========== epoch: [20/100] ==========\n",
      "Epoch [20/100], Step [100/332], Training Loss: 3.6701 | Training accuracy: 68.344%\n",
      "Epoch [20/100], Step [200/332], Training Loss: 3.6096 | Training accuracy: 67.531%\n",
      "Epoch [20/100], Step [300/332], Training Loss: 3.5484 | Training accuracy: 67.115%\n",
      "Epoch:20 Training Loss:3.583 Acc: 67.207\n",
      "--- cost time: 159.4084s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.439 | test_acc: 54.962%\n",
      "************************************\n",
      "\n",
      "========== epoch: [21/100] ==========\n",
      "Epoch [21/100], Step [100/332], Training Loss: 3.4599 | Training accuracy: 67.156%\n",
      "Epoch [21/100], Step [200/332], Training Loss: 3.5468 | Training accuracy: 66.844%\n",
      "Epoch [21/100], Step [300/332], Training Loss: 3.3691 | Training accuracy: 67.729%\n",
      "Epoch:21 Training Loss:3.518 Acc: 67.716\n",
      "--- cost time: 159.1402s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.604 | test_acc: 55.980%\n",
      "************************************\n",
      "\n",
      "========== epoch: [22/100] ==========\n",
      "Epoch [22/100], Step [100/332], Training Loss: 3.5654 | Training accuracy: 69.875%\n",
      "Epoch [22/100], Step [200/332], Training Loss: 3.2828 | Training accuracy: 68.547%\n",
      "Epoch [22/100], Step [300/332], Training Loss: 3.3590 | Training accuracy: 68.052%\n",
      "Epoch:22 Training Loss:3.457 Acc: 68.093\n",
      "--- cost time: 159.3211s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.695 | test_acc: 57.167%\n",
      "************************************\n",
      "\n",
      "========== epoch: [23/100] ==========\n",
      "Epoch [23/100], Step [100/332], Training Loss: 3.3890 | Training accuracy: 70.219%\n",
      "Epoch [23/100], Step [200/332], Training Loss: 3.2768 | Training accuracy: 70.000%\n",
      "Epoch [23/100], Step [300/332], Training Loss: 3.2877 | Training accuracy: 70.219%\n",
      "Epoch:23 Training Loss:3.394 Acc: 70.101\n",
      "--- cost time: 159.6827s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.440 | test_acc: 57.082%\n",
      "************************************\n",
      "\n",
      "========== epoch: [24/100] ==========\n",
      "Epoch [24/100], Step [100/332], Training Loss: 3.6143 | Training accuracy: 70.594%\n",
      "Epoch [24/100], Step [200/332], Training Loss: 3.4376 | Training accuracy: 69.359%\n",
      "Epoch [24/100], Step [300/332], Training Loss: 3.4453 | Training accuracy: 69.354%\n",
      "Epoch:24 Training Loss:3.334 Acc: 69.507\n",
      "--- cost time: 159.8738s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.386 | test_acc: 58.948%\n",
      "************************************\n",
      "\n",
      "========== epoch: [25/100] ==========\n",
      "Epoch [25/100], Step [100/332], Training Loss: 3.3220 | Training accuracy: 72.594%\n",
      "Epoch [25/100], Step [200/332], Training Loss: 3.2291 | Training accuracy: 70.734%\n",
      "Epoch [25/100], Step [300/332], Training Loss: 3.2962 | Training accuracy: 70.552%\n",
      "Epoch:25 Training Loss:3.276 Acc: 70.544\n",
      "--- cost time: 159.5540s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.181 | test_acc: 59.881%\n",
      "************************************\n",
      "\n",
      "========== epoch: [26/100] ==========\n",
      "Epoch [26/100], Step [100/332], Training Loss: 3.2749 | Training accuracy: 72.188%\n",
      "Epoch [26/100], Step [200/332], Training Loss: 3.1257 | Training accuracy: 70.922%\n",
      "Epoch [26/100], Step [300/332], Training Loss: 3.2232 | Training accuracy: 71.510%\n",
      "Epoch:26 Training Loss:3.225 Acc: 71.628\n",
      "--- cost time: 159.6669s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.274 | test_acc: 59.966%\n",
      "************************************\n",
      "\n",
      "========== epoch: [27/100] ==========\n",
      "Epoch [27/100], Step [100/332], Training Loss: 2.9859 | Training accuracy: 71.625%\n",
      "Epoch [27/100], Step [200/332], Training Loss: 3.3364 | Training accuracy: 71.859%\n",
      "Epoch [27/100], Step [300/332], Training Loss: 3.1786 | Training accuracy: 71.854%\n",
      "Epoch:27 Training Loss:3.169 Acc: 71.986\n",
      "--- cost time: 159.7518s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.515 | test_acc: 60.305%\n",
      "************************************\n",
      "\n",
      "========== epoch: [28/100] ==========\n",
      "Epoch [28/100], Step [100/332], Training Loss: 3.1723 | Training accuracy: 73.281%\n",
      "Epoch [28/100], Step [200/332], Training Loss: 3.0984 | Training accuracy: 72.391%\n",
      "Epoch [28/100], Step [300/332], Training Loss: 3.1503 | Training accuracy: 72.625%\n",
      "Epoch:28 Training Loss:3.120 Acc: 72.325\n",
      "--- cost time: 159.4000s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.878 | test_acc: 61.323%\n",
      "************************************\n",
      "\n",
      "========== epoch: [29/100] ==========\n",
      "Epoch [29/100], Step [100/332], Training Loss: 3.1719 | Training accuracy: 72.938%\n",
      "Epoch [29/100], Step [200/332], Training Loss: 2.8775 | Training accuracy: 72.484%\n",
      "Epoch [29/100], Step [300/332], Training Loss: 2.9133 | Training accuracy: 72.875%\n",
      "Epoch:29 Training Loss:3.066 Acc: 72.740\n",
      "--- cost time: 159.1762s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.034 | test_acc: 60.560%\n",
      "************************************\n",
      "\n",
      "========== epoch: [30/100] ==========\n",
      "Epoch [30/100], Step [100/332], Training Loss: 3.0343 | Training accuracy: 75.969%\n",
      "Epoch [30/100], Step [200/332], Training Loss: 2.9740 | Training accuracy: 74.109%\n",
      "Epoch [30/100], Step [300/332], Training Loss: 3.0755 | Training accuracy: 73.750%\n",
      "Epoch:30 Training Loss:3.018 Acc: 73.645\n",
      "--- cost time: 159.4074s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.001 | test_acc: 62.171%\n",
      "************************************\n",
      "\n",
      "========== epoch: [31/100] ==========\n",
      "Epoch [31/100], Step [100/332], Training Loss: 2.8194 | Training accuracy: 73.375%\n",
      "Epoch [31/100], Step [200/332], Training Loss: 3.0373 | Training accuracy: 73.047%\n",
      "Epoch [31/100], Step [300/332], Training Loss: 3.0862 | Training accuracy: 73.458%\n",
      "Epoch:31 Training Loss:2.973 Acc: 73.570\n",
      "--- cost time: 159.6968s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.051 | test_acc: 62.002%\n",
      "************************************\n",
      "\n",
      "========== epoch: [32/100] ==========\n",
      "Epoch [32/100], Step [100/332], Training Loss: 2.9342 | Training accuracy: 74.125%\n",
      "Epoch [32/100], Step [200/332], Training Loss: 2.9474 | Training accuracy: 73.234%\n",
      "Epoch [32/100], Step [300/332], Training Loss: 3.1492 | Training accuracy: 73.500%\n",
      "Epoch:32 Training Loss:2.928 Acc: 73.636\n",
      "--- cost time: 159.7598s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.874 | test_acc: 62.765%\n",
      "************************************\n",
      "\n",
      "========== epoch: [33/100] ==========\n",
      "Epoch [33/100], Step [100/332], Training Loss: 2.8434 | Training accuracy: 73.750%\n",
      "Epoch [33/100], Step [200/332], Training Loss: 2.8473 | Training accuracy: 73.969%\n",
      "Epoch [33/100], Step [300/332], Training Loss: 2.9679 | Training accuracy: 74.490%\n",
      "Epoch:33 Training Loss:2.883 Acc: 74.352\n",
      "--- cost time: 159.6328s ---\n",
      "*************** test ***************\n",
      "test_loss: 3.118 | test_acc: 63.613%\n",
      "************************************\n",
      "\n",
      "========== epoch: [34/100] ==========\n",
      "Epoch [34/100], Step [100/332], Training Loss: 3.0421 | Training accuracy: 73.938%\n",
      "Epoch [34/100], Step [200/332], Training Loss: 2.7758 | Training accuracy: 74.562%\n",
      "Epoch [34/100], Step [300/332], Training Loss: 2.9024 | Training accuracy: 74.750%\n",
      "Epoch:34 Training Loss:2.837 Acc: 74.748\n",
      "--- cost time: 159.4558s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.556 | test_acc: 63.274%\n",
      "************************************\n",
      "\n",
      "========== epoch: [35/100] ==========\n",
      "Epoch [35/100], Step [100/332], Training Loss: 2.8438 | Training accuracy: 75.719%\n",
      "Epoch [35/100], Step [200/332], Training Loss: 3.0987 | Training accuracy: 74.625%\n",
      "Epoch [35/100], Step [300/332], Training Loss: 2.6398 | Training accuracy: 74.948%\n",
      "Epoch:35 Training Loss:2.794 Acc: 75.304\n",
      "--- cost time: 159.2583s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.570 | test_acc: 62.341%\n",
      "************************************\n",
      "\n",
      "========== epoch: [36/100] ==========\n",
      "Epoch [36/100], Step [100/332], Training Loss: 2.7736 | Training accuracy: 76.781%\n",
      "Epoch [36/100], Step [200/332], Training Loss: 2.8341 | Training accuracy: 76.000%\n",
      "Epoch [36/100], Step [300/332], Training Loss: 2.7733 | Training accuracy: 75.896%\n",
      "Epoch:36 Training Loss:2.754 Acc: 75.766\n",
      "--- cost time: 159.3366s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.749 | test_acc: 63.528%\n",
      "************************************\n",
      "\n",
      "========== epoch: [37/100] ==========\n",
      "Epoch [37/100], Step [100/332], Training Loss: 2.8851 | Training accuracy: 77.125%\n",
      "Epoch [37/100], Step [200/332], Training Loss: 2.5806 | Training accuracy: 76.781%\n",
      "Epoch [37/100], Step [300/332], Training Loss: 2.5820 | Training accuracy: 76.458%\n",
      "Epoch:37 Training Loss:2.715 Acc: 76.265\n",
      "--- cost time: 159.5199s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.850 | test_acc: 63.953%\n",
      "************************************\n",
      "\n",
      "========== epoch: [38/100] ==========\n",
      "Epoch [38/100], Step [100/332], Training Loss: 2.7440 | Training accuracy: 76.750%\n",
      "Epoch [38/100], Step [200/332], Training Loss: 2.8864 | Training accuracy: 76.688%\n",
      "Epoch [38/100], Step [300/332], Training Loss: 2.5146 | Training accuracy: 76.208%\n",
      "Epoch:38 Training Loss:2.675 Acc: 75.898\n",
      "--- cost time: 159.3923s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.468 | test_acc: 64.461%\n",
      "************************************\n",
      "\n",
      "========== epoch: [39/100] ==========\n",
      "Epoch [39/100], Step [100/332], Training Loss: 2.6874 | Training accuracy: 76.719%\n",
      "Epoch [39/100], Step [200/332], Training Loss: 2.4833 | Training accuracy: 76.391%\n",
      "Epoch [39/100], Step [300/332], Training Loss: 2.7301 | Training accuracy: 76.479%\n",
      "Epoch:39 Training Loss:2.640 Acc: 76.539\n",
      "--- cost time: 159.4120s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.687 | test_acc: 65.394%\n",
      "************************************\n",
      "\n",
      "========== epoch: [40/100] ==========\n",
      "Epoch [40/100], Step [100/332], Training Loss: 2.7060 | Training accuracy: 76.656%\n",
      "Epoch [40/100], Step [200/332], Training Loss: 2.6506 | Training accuracy: 76.406%\n",
      "Epoch [40/100], Step [300/332], Training Loss: 2.5146 | Training accuracy: 76.562%\n",
      "Epoch:40 Training Loss:2.603 Acc: 76.699\n",
      "--- cost time: 159.5451s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.795 | test_acc: 64.122%\n",
      "************************************\n",
      "\n",
      "========== epoch: [41/100] ==========\n",
      "Epoch [41/100], Step [100/332], Training Loss: 2.7744 | Training accuracy: 77.562%\n",
      "Epoch [41/100], Step [200/332], Training Loss: 2.8238 | Training accuracy: 77.531%\n",
      "Epoch [41/100], Step [300/332], Training Loss: 2.4289 | Training accuracy: 76.708%\n",
      "Epoch:41 Training Loss:2.570 Acc: 76.746\n",
      "--- cost time: 159.4725s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.721 | test_acc: 65.649%\n",
      "************************************\n",
      "\n",
      "========== epoch: [42/100] ==========\n",
      "Epoch [42/100], Step [100/332], Training Loss: 2.6019 | Training accuracy: 76.469%\n",
      "Epoch [42/100], Step [200/332], Training Loss: 2.2732 | Training accuracy: 76.781%\n",
      "Epoch [42/100], Step [300/332], Training Loss: 2.5025 | Training accuracy: 76.958%\n",
      "Epoch:42 Training Loss:2.533 Acc: 76.944\n",
      "--- cost time: 159.4997s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.088 | test_acc: 65.479%\n",
      "************************************\n",
      "\n",
      "========== epoch: [43/100] ==========\n",
      "Epoch [43/100], Step [100/332], Training Loss: 2.5052 | Training accuracy: 77.969%\n",
      "Epoch [43/100], Step [200/332], Training Loss: 2.6804 | Training accuracy: 78.250%\n",
      "Epoch [43/100], Step [300/332], Training Loss: 2.3535 | Training accuracy: 77.885%\n",
      "Epoch:43 Training Loss:2.499 Acc: 77.915\n",
      "--- cost time: 159.2899s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.330 | test_acc: 65.649%\n",
      "************************************\n",
      "\n",
      "========== epoch: [44/100] ==========\n",
      "Epoch [44/100], Step [100/332], Training Loss: 2.4899 | Training accuracy: 76.875%\n",
      "Epoch [44/100], Step [200/332], Training Loss: 2.5847 | Training accuracy: 77.516%\n",
      "Epoch [44/100], Step [300/332], Training Loss: 2.5083 | Training accuracy: 77.490%\n",
      "Epoch:44 Training Loss:2.465 Acc: 77.613\n",
      "--- cost time: 159.3443s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.754 | test_acc: 66.243%\n",
      "************************************\n",
      "\n",
      "========== epoch: [45/100] ==========\n",
      "Epoch [45/100], Step [100/332], Training Loss: 2.3332 | Training accuracy: 77.938%\n",
      "Epoch [45/100], Step [200/332], Training Loss: 2.7658 | Training accuracy: 77.656%\n",
      "Epoch [45/100], Step [300/332], Training Loss: 2.3584 | Training accuracy: 77.865%\n",
      "Epoch:45 Training Loss:2.435 Acc: 78.038\n",
      "--- cost time: 159.3896s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.073 | test_acc: 66.412%\n",
      "************************************\n",
      "\n",
      "========== epoch: [46/100] ==========\n",
      "Epoch [46/100], Step [100/332], Training Loss: 2.5363 | Training accuracy: 78.062%\n",
      "Epoch [46/100], Step [200/332], Training Loss: 2.7848 | Training accuracy: 78.500%\n",
      "Epoch [46/100], Step [300/332], Training Loss: 2.5367 | Training accuracy: 78.552%\n",
      "Epoch:46 Training Loss:2.402 Acc: 78.631\n",
      "--- cost time: 159.3836s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.251 | test_acc: 66.073%\n",
      "************************************\n",
      "\n",
      "========== epoch: [47/100] ==========\n",
      "Epoch [47/100], Step [100/332], Training Loss: 2.3447 | Training accuracy: 77.938%\n",
      "Epoch [47/100], Step [200/332], Training Loss: 2.4136 | Training accuracy: 77.500%\n",
      "Epoch [47/100], Step [300/332], Training Loss: 2.2777 | Training accuracy: 78.229%\n",
      "Epoch:47 Training Loss:2.376 Acc: 78.254\n",
      "--- cost time: 159.6222s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.109 | test_acc: 66.751%\n",
      "************************************\n",
      "\n",
      "========== epoch: [48/100] ==========\n",
      "Epoch [48/100], Step [100/332], Training Loss: 2.3734 | Training accuracy: 79.156%\n",
      "Epoch [48/100], Step [200/332], Training Loss: 2.5612 | Training accuracy: 78.672%\n",
      "Epoch [48/100], Step [300/332], Training Loss: 2.3560 | Training accuracy: 78.458%\n",
      "Epoch:48 Training Loss:2.345 Acc: 78.358\n",
      "--- cost time: 159.8855s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.225 | test_acc: 66.921%\n",
      "************************************\n",
      "\n",
      "========== epoch: [49/100] ==========\n",
      "Epoch [49/100], Step [100/332], Training Loss: 2.4115 | Training accuracy: 79.406%\n",
      "Epoch [49/100], Step [200/332], Training Loss: 2.5679 | Training accuracy: 78.828%\n",
      "Epoch [49/100], Step [300/332], Training Loss: 2.2916 | Training accuracy: 78.781%\n",
      "Epoch:49 Training Loss:2.316 Acc: 79.018\n",
      "--- cost time: 159.9230s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.221 | test_acc: 67.684%\n",
      "************************************\n",
      "\n",
      "========== epoch: [50/100] ==========\n",
      "Epoch [50/100], Step [100/332], Training Loss: 2.2436 | Training accuracy: 79.969%\n",
      "Epoch [50/100], Step [200/332], Training Loss: 2.1922 | Training accuracy: 79.656%\n",
      "Epoch [50/100], Step [300/332], Training Loss: 2.3179 | Training accuracy: 79.240%\n",
      "Epoch:50 Training Loss:2.289 Acc: 79.319\n",
      "--- cost time: 159.8338s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.338 | test_acc: 67.684%\n",
      "************************************\n",
      "\n",
      "========== epoch: [51/100] ==========\n",
      "Epoch [51/100], Step [100/332], Training Loss: 2.3998 | Training accuracy: 78.344%\n",
      "Epoch [51/100], Step [200/332], Training Loss: 1.8206 | Training accuracy: 78.859%\n",
      "Epoch [51/100], Step [300/332], Training Loss: 2.3452 | Training accuracy: 78.865%\n",
      "Epoch:51 Training Loss:2.265 Acc: 78.942\n",
      "--- cost time: 159.1950s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.055 | test_acc: 67.769%\n",
      "************************************\n",
      "\n",
      "========== epoch: [52/100] ==========\n",
      "Epoch [52/100], Step [100/332], Training Loss: 2.1658 | Training accuracy: 79.031%\n",
      "Epoch [52/100], Step [200/332], Training Loss: 2.3333 | Training accuracy: 79.453%\n",
      "Epoch [52/100], Step [300/332], Training Loss: 2.3505 | Training accuracy: 79.521%\n",
      "Epoch:52 Training Loss:2.236 Acc: 79.451\n",
      "--- cost time: 158.9181s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.092 | test_acc: 68.109%\n",
      "************************************\n",
      "\n",
      "========== epoch: [53/100] ==========\n",
      "Epoch [53/100], Step [100/332], Training Loss: 1.9994 | Training accuracy: 79.594%\n",
      "Epoch [53/100], Step [200/332], Training Loss: 1.9372 | Training accuracy: 79.719%\n",
      "Epoch [53/100], Step [300/332], Training Loss: 2.2978 | Training accuracy: 79.927%\n",
      "Epoch:53 Training Loss:2.214 Acc: 79.508\n",
      "--- cost time: 158.8608s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.117 | test_acc: 67.515%\n",
      "************************************\n",
      "\n",
      "========== epoch: [54/100] ==========\n",
      "Epoch [54/100], Step [100/332], Training Loss: 2.1992 | Training accuracy: 78.562%\n",
      "Epoch [54/100], Step [200/332], Training Loss: 2.2777 | Training accuracy: 79.156%\n",
      "Epoch [54/100], Step [300/332], Training Loss: 2.0312 | Training accuracy: 79.469%\n",
      "Epoch:54 Training Loss:2.188 Acc: 79.574\n",
      "--- cost time: 159.3043s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.398 | test_acc: 67.091%\n",
      "************************************\n",
      "\n",
      "========== epoch: [55/100] ==========\n",
      "Epoch [55/100], Step [100/332], Training Loss: 2.0741 | Training accuracy: 80.344%\n",
      "Epoch [55/100], Step [200/332], Training Loss: 2.3620 | Training accuracy: 80.141%\n",
      "Epoch [55/100], Step [300/332], Training Loss: 2.1114 | Training accuracy: 79.635%\n",
      "Epoch:55 Training Loss:2.160 Acc: 79.904\n",
      "--- cost time: 159.4933s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.038 | test_acc: 68.024%\n",
      "************************************\n",
      "\n",
      "========== epoch: [56/100] ==========\n",
      "Epoch [56/100], Step [100/332], Training Loss: 2.0577 | Training accuracy: 81.094%\n",
      "Epoch [56/100], Step [200/332], Training Loss: 2.1650 | Training accuracy: 79.750%\n",
      "Epoch [56/100], Step [300/332], Training Loss: 2.3365 | Training accuracy: 79.990%\n",
      "Epoch:56 Training Loss:2.141 Acc: 79.942\n",
      "--- cost time: 159.5792s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.222 | test_acc: 67.515%\n",
      "************************************\n",
      "\n",
      "========== epoch: [57/100] ==========\n",
      "Epoch [57/100], Step [100/332], Training Loss: 2.0526 | Training accuracy: 81.031%\n",
      "Epoch [57/100], Step [200/332], Training Loss: 2.0545 | Training accuracy: 80.484%\n",
      "Epoch [57/100], Step [300/332], Training Loss: 1.9691 | Training accuracy: 80.229%\n",
      "Epoch:57 Training Loss:2.115 Acc: 80.234\n",
      "--- cost time: 159.7443s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.945 | test_acc: 68.702%\n",
      "************************************\n",
      "\n",
      "========== epoch: [58/100] ==========\n",
      "Epoch [58/100], Step [100/332], Training Loss: 2.1566 | Training accuracy: 80.906%\n",
      "Epoch [58/100], Step [200/332], Training Loss: 2.1218 | Training accuracy: 80.375%\n",
      "Epoch [58/100], Step [300/332], Training Loss: 1.7319 | Training accuracy: 80.583%\n",
      "Epoch:58 Training Loss:2.091 Acc: 80.554\n",
      "--- cost time: 159.4554s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.882 | test_acc: 68.363%\n",
      "************************************\n",
      "\n",
      "========== epoch: [59/100] ==========\n",
      "Epoch [59/100], Step [100/332], Training Loss: 1.8840 | Training accuracy: 82.281%\n",
      "Epoch [59/100], Step [200/332], Training Loss: 2.1111 | Training accuracy: 80.906%\n",
      "Epoch [59/100], Step [300/332], Training Loss: 1.8979 | Training accuracy: 80.490%\n",
      "Epoch:59 Training Loss:2.074 Acc: 80.356\n",
      "--- cost time: 159.0090s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.786 | test_acc: 68.363%\n",
      "************************************\n",
      "\n",
      "========== epoch: [60/100] ==========\n",
      "Epoch [60/100], Step [100/332], Training Loss: 1.7098 | Training accuracy: 81.531%\n",
      "Epoch [60/100], Step [200/332], Training Loss: 2.0894 | Training accuracy: 81.328%\n",
      "Epoch [60/100], Step [300/332], Training Loss: 1.9627 | Training accuracy: 81.042%\n",
      "Epoch:60 Training Loss:2.048 Acc: 81.007\n",
      "--- cost time: 158.6161s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.836 | test_acc: 68.617%\n",
      "************************************\n",
      "\n",
      "========== epoch: [61/100] ==========\n",
      "Epoch [61/100], Step [100/332], Training Loss: 1.9574 | Training accuracy: 81.188%\n",
      "Epoch [61/100], Step [200/332], Training Loss: 2.1173 | Training accuracy: 81.016%\n",
      "Epoch [61/100], Step [300/332], Training Loss: 1.9364 | Training accuracy: 80.906%\n",
      "Epoch:61 Training Loss:2.031 Acc: 80.912\n",
      "--- cost time: 158.8577s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.505 | test_acc: 68.448%\n",
      "************************************\n",
      "\n",
      "========== epoch: [62/100] ==========\n",
      "Epoch [62/100], Step [100/332], Training Loss: 1.9579 | Training accuracy: 81.062%\n",
      "Epoch [62/100], Step [200/332], Training Loss: 2.0052 | Training accuracy: 80.906%\n",
      "Epoch [62/100], Step [300/332], Training Loss: 2.1805 | Training accuracy: 81.146%\n",
      "Epoch:62 Training Loss:2.007 Acc: 81.129\n",
      "--- cost time: 158.7618s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.903 | test_acc: 68.109%\n",
      "************************************\n",
      "\n",
      "========== epoch: [63/100] ==========\n",
      "Epoch [63/100], Step [100/332], Training Loss: 1.9128 | Training accuracy: 81.406%\n",
      "Epoch [63/100], Step [200/332], Training Loss: 2.1876 | Training accuracy: 81.125%\n",
      "Epoch [63/100], Step [300/332], Training Loss: 1.8884 | Training accuracy: 81.271%\n",
      "Epoch:63 Training Loss:1.990 Acc: 81.299\n",
      "--- cost time: 158.7989s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.674 | test_acc: 68.363%\n",
      "************************************\n",
      "\n",
      "========== epoch: [64/100] ==========\n",
      "Epoch [64/100], Step [100/332], Training Loss: 2.1233 | Training accuracy: 81.625%\n",
      "Epoch [64/100], Step [200/332], Training Loss: 1.9669 | Training accuracy: 81.266%\n",
      "Epoch [64/100], Step [300/332], Training Loss: 2.0341 | Training accuracy: 81.219%\n",
      "Epoch:64 Training Loss:1.970 Acc: 81.110\n",
      "--- cost time: 159.1489s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.804 | test_acc: 68.702%\n",
      "************************************\n",
      "\n",
      "========== epoch: [65/100] ==========\n",
      "Epoch [65/100], Step [100/332], Training Loss: 1.8759 | Training accuracy: 81.312%\n",
      "Epoch [65/100], Step [200/332], Training Loss: 1.9210 | Training accuracy: 80.969%\n",
      "Epoch [65/100], Step [300/332], Training Loss: 1.9941 | Training accuracy: 81.219%\n",
      "Epoch:65 Training Loss:1.950 Acc: 81.223\n",
      "--- cost time: 159.2566s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.655 | test_acc: 69.042%\n",
      "************************************\n",
      "\n",
      "========== epoch: [66/100] ==========\n",
      "Epoch [66/100], Step [100/332], Training Loss: 2.0429 | Training accuracy: 81.875%\n",
      "Epoch [66/100], Step [200/332], Training Loss: 1.8950 | Training accuracy: 82.047%\n",
      "Epoch [66/100], Step [300/332], Training Loss: 2.0817 | Training accuracy: 81.385%\n",
      "Epoch:66 Training Loss:1.932 Acc: 81.582\n",
      "--- cost time: 159.0596s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.588 | test_acc: 68.957%\n",
      "************************************\n",
      "\n",
      "========== epoch: [67/100] ==========\n",
      "Epoch [67/100], Step [100/332], Training Loss: 1.9311 | Training accuracy: 81.719%\n",
      "Epoch [67/100], Step [200/332], Training Loss: 1.8684 | Training accuracy: 81.672%\n",
      "Epoch [67/100], Step [300/332], Training Loss: 1.9418 | Training accuracy: 82.021%\n",
      "Epoch:67 Training Loss:1.912 Acc: 81.874\n",
      "--- cost time: 158.8215s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.159 | test_acc: 69.975%\n",
      "************************************\n",
      "\n",
      "========== epoch: [68/100] ==========\n",
      "Epoch [68/100], Step [100/332], Training Loss: 1.9146 | Training accuracy: 82.406%\n",
      "Epoch [68/100], Step [200/332], Training Loss: 1.9796 | Training accuracy: 82.297%\n",
      "Epoch [68/100], Step [300/332], Training Loss: 1.9589 | Training accuracy: 81.969%\n",
      "Epoch:68 Training Loss:1.896 Acc: 81.808\n",
      "--- cost time: 158.6521s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.793 | test_acc: 69.635%\n",
      "************************************\n",
      "\n",
      "========== epoch: [69/100] ==========\n",
      "Epoch [69/100], Step [100/332], Training Loss: 2.1117 | Training accuracy: 82.406%\n",
      "Epoch [69/100], Step [200/332], Training Loss: 2.0620 | Training accuracy: 81.891%\n",
      "Epoch [69/100], Step [300/332], Training Loss: 1.9494 | Training accuracy: 81.823%\n",
      "Epoch:69 Training Loss:1.878 Acc: 81.921\n",
      "--- cost time: 158.5245s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.687 | test_acc: 69.890%\n",
      "************************************\n",
      "\n",
      "========== epoch: [70/100] ==========\n",
      "Epoch [70/100], Step [100/332], Training Loss: 1.8448 | Training accuracy: 82.375%\n",
      "Epoch [70/100], Step [200/332], Training Loss: 1.8956 | Training accuracy: 81.969%\n",
      "Epoch [70/100], Step [300/332], Training Loss: 1.6244 | Training accuracy: 81.844%\n",
      "Epoch:70 Training Loss:1.863 Acc: 81.789\n",
      "--- cost time: 158.6712s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.741 | test_acc: 68.787%\n",
      "************************************\n",
      "\n",
      "========== epoch: [71/100] ==========\n",
      "Epoch [71/100], Step [100/332], Training Loss: 1.8328 | Training accuracy: 82.750%\n",
      "Epoch [71/100], Step [200/332], Training Loss: 1.7931 | Training accuracy: 82.562%\n",
      "Epoch [71/100], Step [300/332], Training Loss: 1.7494 | Training accuracy: 82.073%\n",
      "Epoch:71 Training Loss:1.850 Acc: 81.987\n",
      "--- cost time: 158.6754s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.797 | test_acc: 69.720%\n",
      "************************************\n",
      "\n",
      "========== epoch: [72/100] ==========\n",
      "Epoch [72/100], Step [100/332], Training Loss: 1.8226 | Training accuracy: 81.875%\n",
      "Epoch [72/100], Step [200/332], Training Loss: 1.9590 | Training accuracy: 82.516%\n",
      "Epoch [72/100], Step [300/332], Training Loss: 1.8689 | Training accuracy: 82.500%\n",
      "Epoch:72 Training Loss:1.831 Acc: 82.251\n",
      "--- cost time: 158.8167s ---\n",
      "*************** test ***************\n",
      "test_loss: 2.171 | test_acc: 68.957%\n",
      "************************************\n",
      "\n",
      "========== epoch: [73/100] ==========\n",
      "Epoch [73/100], Step [100/332], Training Loss: 1.6986 | Training accuracy: 83.250%\n",
      "Epoch [73/100], Step [200/332], Training Loss: 1.8092 | Training accuracy: 82.828%\n",
      "Epoch [73/100], Step [300/332], Training Loss: 1.9585 | Training accuracy: 82.312%\n",
      "Epoch:73 Training Loss:1.813 Acc: 82.289\n",
      "--- cost time: 158.5811s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.855 | test_acc: 70.314%\n",
      "************************************\n",
      "\n",
      "========== epoch: [74/100] ==========\n",
      "Epoch [74/100], Step [100/332], Training Loss: 1.7490 | Training accuracy: 83.438%\n",
      "Epoch [74/100], Step [200/332], Training Loss: 1.6136 | Training accuracy: 83.219%\n",
      "Epoch [74/100], Step [300/332], Training Loss: 1.6904 | Training accuracy: 82.938%\n",
      "Epoch:74 Training Loss:1.796 Acc: 82.930\n",
      "--- cost time: 158.5149s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.961 | test_acc: 69.975%\n",
      "************************************\n",
      "\n",
      "========== epoch: [75/100] ==========\n",
      "Epoch [75/100], Step [100/332], Training Loss: 1.7333 | Training accuracy: 82.438%\n",
      "Epoch [75/100], Step [200/332], Training Loss: 1.4717 | Training accuracy: 82.484%\n",
      "Epoch [75/100], Step [300/332], Training Loss: 2.0267 | Training accuracy: 82.260%\n",
      "Epoch:75 Training Loss:1.786 Acc: 82.307\n",
      "--- cost time: 158.7267s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.437 | test_acc: 69.720%\n",
      "************************************\n",
      "\n",
      "========== epoch: [76/100] ==========\n",
      "Epoch [76/100], Step [100/332], Training Loss: 1.8754 | Training accuracy: 84.281%\n",
      "Epoch [76/100], Step [200/332], Training Loss: 1.6418 | Training accuracy: 83.469%\n",
      "Epoch [76/100], Step [300/332], Training Loss: 1.5559 | Training accuracy: 82.917%\n",
      "Epoch:76 Training Loss:1.767 Acc: 82.666\n",
      "--- cost time: 158.9364s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.963 | test_acc: 69.975%\n",
      "************************************\n",
      "\n",
      "========== epoch: [77/100] ==========\n",
      "Epoch [77/100], Step [100/332], Training Loss: 1.6084 | Training accuracy: 82.469%\n",
      "Epoch [77/100], Step [200/332], Training Loss: 1.6923 | Training accuracy: 82.891%\n",
      "Epoch [77/100], Step [300/332], Training Loss: 1.4921 | Training accuracy: 83.135%\n",
      "Epoch:77 Training Loss:1.754 Acc: 82.911\n",
      "--- cost time: 158.7553s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.908 | test_acc: 70.399%\n",
      "************************************\n",
      "\n",
      "========== epoch: [78/100] ==========\n",
      "Epoch [78/100], Step [100/332], Training Loss: 2.0644 | Training accuracy: 81.219%\n",
      "Epoch [78/100], Step [200/332], Training Loss: 1.5797 | Training accuracy: 82.094%\n",
      "Epoch [78/100], Step [300/332], Training Loss: 1.6165 | Training accuracy: 82.812%\n",
      "Epoch:78 Training Loss:1.740 Acc: 82.864\n",
      "--- cost time: 158.4908s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.520 | test_acc: 70.059%\n",
      "************************************\n",
      "\n",
      "========== epoch: [79/100] ==========\n",
      "Epoch [79/100], Step [100/332], Training Loss: 1.7124 | Training accuracy: 83.469%\n",
      "Epoch [79/100], Step [200/332], Training Loss: 1.5899 | Training accuracy: 82.844%\n",
      "Epoch [79/100], Step [300/332], Training Loss: 1.9443 | Training accuracy: 82.844%\n",
      "Epoch:79 Training Loss:1.725 Acc: 82.854\n",
      "--- cost time: 158.8065s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.738 | test_acc: 70.568%\n",
      "************************************\n",
      "\n",
      "========== epoch: [80/100] ==========\n",
      "Epoch [80/100], Step [300/332], Training Loss: 1.7589 | Training accuracy: 83.188%\n",
      "Epoch:80 Training Loss:1.711 Acc: 83.128\n",
      "--- cost time: 158.8406s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.939 | test_acc: 70.823%\n",
      "************************************\n",
      "\n",
      "========== epoch: [81/100] ==========\n",
      "Epoch [81/100], Step [100/332], Training Loss: 1.7288 | Training accuracy: 83.500%\n",
      "Epoch [81/100], Step [200/332], Training Loss: 1.9054 | Training accuracy: 83.531%\n",
      "Epoch [81/100], Step [300/332], Training Loss: 1.6814 | Training accuracy: 83.229%\n",
      "Epoch:81 Training Loss:1.701 Acc: 83.184\n",
      "--- cost time: 158.9125s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.777 | test_acc: 70.738%\n",
      "************************************\n",
      "\n",
      "========== epoch: [82/100] ==========\n",
      "Epoch [82/100], Step [100/332], Training Loss: 1.4844 | Training accuracy: 83.062%\n",
      "Epoch [82/100], Step [200/332], Training Loss: 1.5454 | Training accuracy: 83.156%\n",
      "Epoch [82/100], Step [300/332], Training Loss: 1.6858 | Training accuracy: 83.229%\n",
      "Epoch:82 Training Loss:1.687 Acc: 83.033\n",
      "--- cost time: 158.8787s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.707 | test_acc: 69.805%\n",
      "************************************\n",
      "\n",
      "========== epoch: [83/100] ==========\n",
      "Epoch [83/100], Step [100/332], Training Loss: 1.5785 | Training accuracy: 83.281%\n",
      "Epoch [83/100], Step [200/332], Training Loss: 1.6839 | Training accuracy: 83.156%\n",
      "Epoch [83/100], Step [300/332], Training Loss: 1.4035 | Training accuracy: 82.885%\n",
      "Epoch:83 Training Loss:1.674 Acc: 82.845\n",
      "--- cost time: 159.2440s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.580 | test_acc: 71.162%\n",
      "************************************\n",
      "\n",
      "========== epoch: [84/100] ==========\n",
      "Epoch [84/100], Step [100/332], Training Loss: 1.7751 | Training accuracy: 84.062%\n",
      "Epoch [84/100], Step [200/332], Training Loss: 1.6941 | Training accuracy: 83.359%\n",
      "Epoch [84/100], Step [300/332], Training Loss: 1.7051 | Training accuracy: 83.417%\n",
      "Epoch:84 Training Loss:1.663 Acc: 83.297\n",
      "--- cost time: 158.7811s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.741 | test_acc: 70.144%\n",
      "************************************\n",
      "\n",
      "========== epoch: [85/100] ==========\n",
      "Epoch [85/100], Step [100/332], Training Loss: 1.7465 | Training accuracy: 83.875%\n",
      "Epoch [85/100], Step [200/332], Training Loss: 1.4636 | Training accuracy: 83.469%\n",
      "Epoch [85/100], Step [300/332], Training Loss: 1.7422 | Training accuracy: 83.260%\n",
      "Epoch:85 Training Loss:1.648 Acc: 83.241\n",
      "--- cost time: 158.6798s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.338 | test_acc: 70.823%\n",
      "************************************\n",
      "\n",
      "========== epoch: [86/100] ==========\n",
      "Epoch [86/100], Step [100/332], Training Loss: 1.9356 | Training accuracy: 82.656%\n",
      "Epoch [86/100], Step [200/332], Training Loss: 1.6656 | Training accuracy: 82.812%\n",
      "Epoch [86/100], Step [300/332], Training Loss: 1.5286 | Training accuracy: 83.219%\n",
      "Epoch:86 Training Loss:1.638 Acc: 83.194\n",
      "--- cost time: 158.6450s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.762 | test_acc: 70.144%\n",
      "************************************\n",
      "\n",
      "========== epoch: [87/100] ==========\n",
      "Epoch [87/100], Step [100/332], Training Loss: 1.3282 | Training accuracy: 84.031%\n",
      "Epoch [87/100], Step [200/332], Training Loss: 1.6976 | Training accuracy: 83.938%\n",
      "Epoch [87/100], Step [300/332], Training Loss: 1.4465 | Training accuracy: 83.729%\n",
      "Epoch:87 Training Loss:1.625 Acc: 83.571\n",
      "--- cost time: 158.6370s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.129 | test_acc: 71.077%\n",
      "************************************\n",
      "\n",
      "========== epoch: [88/100] ==========\n",
      "Epoch [88/100], Step [100/332], Training Loss: 1.5229 | Training accuracy: 82.875%\n",
      "Epoch [88/100], Step [200/332], Training Loss: 1.6601 | Training accuracy: 82.859%\n",
      "Epoch [88/100], Step [300/332], Training Loss: 1.6403 | Training accuracy: 83.427%\n",
      "Epoch:88 Training Loss:1.611 Acc: 83.552\n",
      "--- cost time: 158.9424s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.261 | test_acc: 70.823%\n",
      "************************************\n",
      "\n",
      "========== epoch: [89/100] ==========\n",
      "Epoch [89/100], Step [100/332], Training Loss: 1.4475 | Training accuracy: 83.469%\n",
      "Epoch [89/100], Step [200/332], Training Loss: 1.3279 | Training accuracy: 83.234%\n",
      "Epoch [89/100], Step [300/332], Training Loss: 1.8211 | Training accuracy: 83.531%\n",
      "Epoch:89 Training Loss:1.604 Acc: 83.552\n",
      "--- cost time: 159.1280s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.371 | test_acc: 70.314%\n",
      "************************************\n",
      "\n",
      "========== epoch: [90/100] ==========\n",
      "Epoch [90/100], Step [100/332], Training Loss: 1.4415 | Training accuracy: 84.156%\n",
      "Epoch [90/100], Step [200/332], Training Loss: 1.5379 | Training accuracy: 83.953%\n",
      "Epoch [90/100], Step [300/332], Training Loss: 1.6917 | Training accuracy: 83.667%\n",
      "Epoch:90 Training Loss:1.591 Acc: 83.816\n",
      "--- cost time: 159.2725s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.590 | test_acc: 69.975%\n",
      "************************************\n",
      "\n",
      "========== epoch: [91/100] ==========\n",
      "Epoch [91/100], Step [100/332], Training Loss: 1.5661 | Training accuracy: 83.906%\n",
      "Epoch [91/100], Step [200/332], Training Loss: 1.6059 | Training accuracy: 83.688%\n",
      "Epoch [91/100], Step [300/332], Training Loss: 1.8230 | Training accuracy: 83.635%\n",
      "Epoch:91 Training Loss:1.579 Acc: 83.910\n",
      "--- cost time: 159.1670s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.462 | test_acc: 70.992%\n",
      "************************************\n",
      "\n",
      "========== epoch: [92/100] ==========\n",
      "Epoch [92/100], Step [100/332], Training Loss: 1.8113 | Training accuracy: 84.562%\n",
      "Epoch [92/100], Step [200/332], Training Loss: 1.2982 | Training accuracy: 84.906%\n",
      "Epoch [92/100], Step [300/332], Training Loss: 1.7264 | Training accuracy: 84.500%\n",
      "Epoch:92 Training Loss:1.568 Acc: 84.202\n",
      "--- cost time: 158.7473s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.711 | test_acc: 70.483%\n",
      "************************************\n",
      "\n",
      "========== epoch: [93/100] ==========\n",
      "Epoch [93/100], Step [100/332], Training Loss: 1.6548 | Training accuracy: 83.781%\n",
      "Epoch [93/100], Step [200/332], Training Loss: 1.7957 | Training accuracy: 84.203%\n",
      "Epoch [93/100], Step [300/332], Training Loss: 1.5880 | Training accuracy: 84.135%\n",
      "Epoch:93 Training Loss:1.560 Acc: 84.089\n",
      "--- cost time: 158.5917s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.756 | test_acc: 70.568%\n",
      "************************************\n",
      "\n",
      "========== epoch: [94/100] ==========\n",
      "Epoch [94/100], Step [100/332], Training Loss: 1.8797 | Training accuracy: 84.125%\n",
      "Epoch [94/100], Step [200/332], Training Loss: 1.4035 | Training accuracy: 84.312%\n",
      "Epoch [94/100], Step [300/332], Training Loss: 1.2745 | Training accuracy: 83.958%\n",
      "Epoch:94 Training Loss:1.549 Acc: 83.919\n",
      "--- cost time: 158.9402s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.819 | test_acc: 70.738%\n",
      "************************************\n",
      "\n",
      "========== epoch: [95/100] ==========\n",
      "Epoch [95/100], Step [100/332], Training Loss: 1.6698 | Training accuracy: 85.125%\n",
      "Epoch [95/100], Step [200/332], Training Loss: 1.5900 | Training accuracy: 84.312%\n",
      "Epoch [95/100], Step [300/332], Training Loss: 1.6455 | Training accuracy: 84.323%\n",
      "Epoch:95 Training Loss:1.539 Acc: 84.230\n",
      "--- cost time: 158.9263s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.883 | test_acc: 71.332%\n",
      "************************************\n",
      "\n",
      "========== epoch: [96/100] ==========\n",
      "Epoch [96/100], Step [100/332], Training Loss: 1.7611 | Training accuracy: 84.719%\n",
      "Epoch [96/100], Step [200/332], Training Loss: 1.3175 | Training accuracy: 84.438%\n",
      "Epoch [96/100], Step [300/332], Training Loss: 1.2942 | Training accuracy: 84.177%\n",
      "Epoch:96 Training Loss:1.531 Acc: 84.089\n",
      "--- cost time: 159.0465s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.683 | test_acc: 70.823%\n",
      "************************************\n",
      "\n",
      "========== epoch: [97/100] ==========\n",
      "Epoch [97/100], Step [100/332], Training Loss: 1.5509 | Training accuracy: 84.344%\n",
      "Epoch [97/100], Step [200/332], Training Loss: 1.4578 | Training accuracy: 84.516%\n",
      "Epoch [97/100], Step [300/332], Training Loss: 1.6841 | Training accuracy: 84.594%\n",
      "Epoch:97 Training Loss:1.518 Acc: 84.400\n",
      "--- cost time: 158.8683s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.824 | test_acc: 70.908%\n",
      "************************************\n",
      "\n",
      "========== epoch: [98/100] ==========\n",
      "Epoch [98/100], Step [100/332], Training Loss: 1.4072 | Training accuracy: 84.594%\n",
      "Epoch [98/100], Step [200/332], Training Loss: 1.3123 | Training accuracy: 84.594%\n",
      "Epoch [98/100], Step [300/332], Training Loss: 1.5719 | Training accuracy: 84.615%\n",
      "Epoch:98 Training Loss:1.507 Acc: 84.447\n",
      "--- cost time: 158.6442s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.653 | test_acc: 71.416%\n",
      "************************************\n",
      "\n",
      "========== epoch: [99/100] ==========\n",
      "Epoch [99/100], Step [100/332], Training Loss: 1.2609 | Training accuracy: 85.250%\n",
      "Epoch [99/100], Step [200/332], Training Loss: 1.1861 | Training accuracy: 84.219%\n",
      "Epoch [99/100], Step [300/332], Training Loss: 1.5825 | Training accuracy: 84.531%\n",
      "Epoch:99 Training Loss:1.500 Acc: 84.485\n",
      "--- cost time: 158.6819s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.410 | test_acc: 70.144%\n",
      "************************************\n",
      "\n",
      "========== epoch: [100/100] ==========\n",
      "Epoch [100/100], Step [100/332], Training Loss: 1.6215 | Training accuracy: 84.031%\n",
      "Epoch [100/100], Step [200/332], Training Loss: 1.6626 | Training accuracy: 84.578%\n",
      "Epoch [100/100], Step [300/332], Training Loss: 1.5017 | Training accuracy: 84.510%\n",
      "Epoch:100 Training Loss:1.493 Acc: 84.353\n",
      "--- cost time: 158.7381s ---\n",
      "*************** test ***************\n",
      "test_loss: 1.826 | test_acc: 71.077%\n",
      "************************************\n",
      "\n",
      "Training finished! Model saved to BCNN_savedModel_71.416%.pth\n"
     ]
    }
   ],
   "source": [
    "net = BCNN().to(device)\n",
    "\n",
    "# 损失函数使用交叉熵损失函数\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "# 优化器使用SGD\n",
    "optimizer = torch.optim.SGD(net.fc.parameters(),\n",
    "                            lr=BASE_LEARNING_RATE,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=WEIGHT_DECAY)\n",
    "# 调整学习率\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, verbose=True, threshold=1e-4)\n",
    "\n",
    "# 训练模型\n",
    "print('Start Training')\n",
    "total_step = len(train_loader)\n",
    "best_model_wts = net.state_dict()\n",
    "best_acc = 0.0\n",
    "record_train = list()\n",
    "record_test = list()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # 保存训练过程文件\n",
    "    mylog = open('BCNN_training_progress.log', mode='a', encoding='utf-8')\n",
    "    print(\"========== epoch: [{}/{}] ==========\".format(epoch + 1, EPOCHS), file=mylog)\n",
    "    print(\"========== epoch: [{}/{}] ==========\".format(epoch + 1, EPOCHS))\n",
    "    epoch_loss = []\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    start = time.time()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # 数据转为cuda\n",
    "        images = torch.autograd.Variable(images.cuda())\n",
    "        labels = torch.autograd.Variable(labels.cuda())\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 前向传播\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss.append(loss.data)\n",
    "        # 后向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 预测\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        num_total += labels.size(0)\n",
    "        num_correct += torch.sum(prediction == labels.data)\n",
    "        train_acc = 100.0 * num_correct / num_total\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Training Loss: {:.4f} | Training accuracy: {:6.3f}%'.\n",
    "                  format(epoch + 1, EPOCHS, i + 1, total_step, loss.item(), train_acc))\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Training Loss: {:.4f} | Training accuracy: {:6.3f}%'.\n",
    "                  format(epoch + 1, EPOCHS, i + 1, total_step, loss.item(), train_acc), file=mylog)\n",
    "\n",
    "    record_train.append(train_acc)\n",
    "    print('Epoch:%d Training Loss:%.03f Acc: %.03f' % (epoch + 1, sum(epoch_loss) / len(epoch_loss), train_acc))\n",
    "    print('Epoch:%d Training Loss:%.03f Acc: %.03f' % (epoch + 1, sum(epoch_loss) / len(epoch_loss), train_acc), file=mylog)\n",
    "    print(\"--- cost time: {:.4f}s ---\".format(time.time() - start))\n",
    "    print(\"--- cost time: {:.4f}s ---\".format(time.time() - start), file=mylog)\n",
    "\n",
    "    # 在测试集上进行测试\n",
    "    print(\"*************** test ***************\")\n",
    "    print(\"*************** test ***************\", file=mylog)\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for images, labels in test_loader:\n",
    "            net.eval()\n",
    "            images = torch.autograd.Variable(images.cuda())\n",
    "            labels = torch.autograd.Variable(labels.cuda())\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, prediction = torch.max(outputs.data, 1)\n",
    "            num_total += labels.size(0)\n",
    "            num_correct += torch.sum(prediction == labels.data).item()\n",
    "\n",
    "    test_acc = 100 * num_correct / num_total\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        best_model_wts = net.state_dict()\n",
    "    record_test.append(test_acc)\n",
    "    print(\"test_loss: {:.3f} | test_acc: {:6.3f}%\".format(loss.item(), test_acc))\n",
    "    print(\"test_loss: {:.3f} | test_acc: {:6.3f}%\".format(loss.item(), test_acc), file=mylog)\n",
    "    print(\"************************************\\n\")\n",
    "    print(\"************************************\\n\", file=mylog)\n",
    "    mylog.close()\n",
    "\n",
    "# 保存最好的模型\n",
    "net.load_state_dict(best_model_wts)\n",
    "model_name = \"BCNN_savedModel_{:6.3f}%.pth\".format(best_acc)\n",
    "torch.save(net, model_name)\n",
    "print(\"Training finished! Model saved to \" + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可视化训练过程\n",
    "train_acc_result = []\n",
    "for record in record_train:\n",
    "    train_acc_result.append(record.cuda().data.cpu().numpy())\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.plot(range(1, len(train_acc_result) + 1), train_acc_result, label=\"train acc\")\n",
    "if record_test is not None:\n",
    "    plt.plot(range(1, len(record_test) + 1), record_test, label=\"test acc\")\n",
    "\n",
    "plt.legend(loc=4)\n",
    "plt.title(\"learning curve\")\n",
    "plt.xticks(range(0, len(train_acc_result) + 1, 5))\n",
    "plt.yticks(range(0, 101, 5))\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
